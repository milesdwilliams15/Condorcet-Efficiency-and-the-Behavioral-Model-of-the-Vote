---
title: "A Replication of 'Condorcet Efficiency and the Behavioral Model of the Vote'"
author: "Miles Williams"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

Adams (1997) proposed a model of a given voter $i$'s policy losses with respect to a given candidate $K$ via the following quadratic loss function
$$P_i(K)=\sum_{j\in m}b_{ij}(x_{ij}-k_j)^2,\tag{1}$$
where a voter's utility is then given as
$$U_i(K)=-\sum_{j\in m}b_{ij}(x_{ij}-k_j)^2 + \mu_{iK}.\tag{2}$$
The policy position of voter $i$ along policy issue $j$ lies at some point in the interval $[0,1]$. A candidate $k$ holds some policy position within this same interval for policy $j$. The above equations imply that a voter's policy losses will be minimized, and utility maximized, by selecting the candidate whose policy position is nearest to her own. The coefficients $b_{ij}$ denote the salience of policy $j$ to voter $i$. If $b_{ij}=0$ a voter is completely indifferent about candidate $k$'s position on policy $j$. However, as $b_{ij}\rightarrow\infty$, policy $j$ increasingly becomes the most relevant factor determining the utility voter $i$ receives with respect to candidate $k$. The term $\mu_{iK}$ is a random variable that denotes non-policy related reasons voter $i$ may prefer to vote for a given candidate. This random variable is generated from a type I extreme value distribution (also known as a Gumbel distribution). The less salient a given policy is to a given voter, the more a voter's utility is determined by the idiosyncratic motivations captured by $\mu_{iK}$.

Figure 1 shows results from a replication of the simulation used by Adams (1997) to demonstrate how the above model of voter behavior works. Following Adams (1997), I generated 25,001 random policy positions for 25,001 voters along a single policy dimension $J$, pulling values from a uniform distribution. Adams (1997) further randomly generated policy positions for three candidates (A, B, and C) whose policy positions were .39, .47, and .71, respectively. I use these same candidate positions in my replication. I run 4 simulations, where the same voter and candidate preferences are used across simulations but where $b_J$ is varied. For each simulation I assume that policy $J$'s salience is constant across voters. In the first simulation $b_J=100$, for the second 20, for the third 3, and for the last 1.

![Results from Monte Carlo Simulation of Candidate Vote Shares per Voters' Locations along Policy $J$.](Rplot.pdf)

